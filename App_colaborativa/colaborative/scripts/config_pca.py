# -*- coding: utf-8 -*-
"""
Configuraci√≥n del Sistema RAG Enriquecido con PCA
Define rutas, par√°metros y configuraciones globales
"""

import os
from pathlib import Path

# ==========================================================
# üîπ CONFIGURACI√ìN DE DIRECTORIOS
# ==========================================================
BASE_DIR = Path("colaborative")
DATA_DIR = BASE_DIR / "data"
MODELS_DIR = BASE_DIR / "models"
SCRIPTS_DIR = BASE_DIR / "scripts"

# Directorios de datos
PDFS_DIR = DATA_DIR / "pdfs"
PDFS_GENERAL = PDFS_DIR / "general"
PDFS_CIVIL = PDFS_DIR / "civil"
INDEX_DIR = DATA_DIR / "index"
LOGS_DIR = DATA_DIR / "logs"
CHUNKS_DIR = DATA_DIR / "chunks"

# Crear directorios si no existen
for directory in [DATA_DIR, PDFS_GENERAL, PDFS_CIVIL, INDEX_DIR, LOGS_DIR, CHUNKS_DIR]:
    directory.mkdir(parents=True, exist_ok=True)

# ==========================================================
# üîπ CONFIGURACI√ìN DE BASES DE DATOS
# ==========================================================
# Base de datos tradicional de autoaprendizaje
DB_AUTOAPRENDIZAJE = DATA_DIR / "autoaprendizaje.db"

# Base de datos de perfiles cognitivos (PCA)
DB_PERFILES = DATA_DIR / "perfiles.db"

# √çndices FAISS
FAISS_A_INDEX = INDEX_DIR / "vector_index.faiss"  # Contenido tradicional
FAISS_B_INDEX = DATA_DIR / "faiss_profiles.index"  # Perfiles cognitivos
FAISS_B_META = DATA_DIR / "faiss_profiles_meta.json"

# Historial de refinamiento
HISTORIAL_REFINAMIENTO = LOGS_DIR / "refinamiento.json"

# ==========================================================
# üîπ CONFIGURACI√ìN DE MODELOS
# ==========================================================
# Modelos de embeddings
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
EMBEDDING_PROFILES_MODEL = "sentence-transformers/all-MiniLM-L6-v2"  # Mismo para consistencia

# Modelos de generaci√≥n
GENERATOR_MODEL_LOCAL = "google/flan-t5-base"
GENERATOR_MODEL_CLOUD = "gemini-2.5-pro"  # Requiere API key

# Modelo NER
NER_MODEL = "mrm8488/bert-spanish-cased-finetuned-ner"

# ==========================================================
# üîπ CONFIGURACI√ìN DE PROCESAMIENTO
# ==========================================================
# Par√°metros de chunking
DEFAULT_CHUNK_SIZE = 800
DEFAULT_CHUNK_OVERLAP = 100

# Par√°metros de b√∫squeda
DEFAULT_K_SEARCH = 5  # Fragmentos tradicionales
DEFAULT_K_PROFILES = 6  # Perfiles cognitivos

# Par√°metros de generaci√≥n
MAX_NEW_TOKENS = 256
TEMPERATURE = 0.1

# ==========================================================
# üîπ CONFIGURACI√ìN DE APIs EXTERNAS
# ==========================================================
# Google Gemini
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
USE_GEMINI = os.getenv("USE_GEMINI", "True").lower() == "true"

# HuggingFace
HF_TOKEN = os.getenv("HF_TOKEN")

# ==========================================================
# üîπ CONFIGURACI√ìN DE LOGGING
# ==========================================================
ENABLE_VERBOSE_LOGGING = True
LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR
LOG_TO_FILE = True

# ==========================================================
# üîπ CONFIGURACI√ìN DEL SISTEMA PCA
# ==========================================================
# Marcos de referencia detectables
MARCOS_REFERENCIA = {
    "socio_filosofico": ["foucault", "habermas", "weber", "durkheim", "bourdieu", "luhmann"],
    "economico_liberal": ["hayek", "friedman", "keynes", "smith", "schumpeter", "becker"],
    "juridico_garantista": ["kelsen", "hart", "ross", "dworkin", "ferrajoli", "rawls", "alexy"],
    "critico_materialista": ["marx", "gramsci", "lenin", "althusser"],
    "filosofico_existencial": ["arendt", "heidegger", "husserl", "nietzsche"],
    "juridico_dogmatico": ["constitucional", "penal", "civil", "comercial", "procesal", "administrativo"]
}

# Estrategias intelectuales
ESTRATEGIAS_DETECTABLES = [
    "Comparativa", "Propositiva", "Anal√≠tica", "Cr√≠tica", "Expositiva"
]

# Metodolog√≠as jur√≠dicas
METODOLOGIAS_JURIDICAS = [
    "Jurisprudencial", "Doctrinaria", "Constitucional", "Normativa", "Comparada", "Hist√≥rica", "Dogm√°tica general"
]

# ==========================================================
# üîπ CONFIGURACI√ìN DE LA WEBAPP
# ==========================================================
FLASK_HOST = "127.0.0.1"
FLASK_PORT = 5002
FLASK_DEBUG = False

# Rutas de templates
TEMPLATES_DIR = BASE_DIR / "templates"
STATIC_DIR = BASE_DIR / "static"

# ==========================================================
# üîπ FUNCIONES DE VALIDACI√ìN
# ==========================================================
def validate_config():
    """Valida la configuraci√≥n y dependencias"""
    issues = []
    
    # Verificar directorios cr√≠ticos
    if not MODELS_DIR.exists():
        issues.append(f"Directorio de modelos no encontrado: {MODELS_DIR}")
    
    # Verificar API keys si est√°n configuradas
    if USE_GEMINI and not GOOGLE_API_KEY:
        issues.append("USE_GEMINI=True pero GOOGLE_API_KEY no est√° configurada")
    
    # Verificar modelos locales
    embedding_path = MODELS_DIR / "embeddings" / "all-MiniLM-L6-v2"
    if not embedding_path.exists():
        issues.append(f"Modelo de embeddings no encontrado: {embedding_path}")
    
    generator_path = MODELS_DIR / "generator" / "flan-t5-base"
    if not generator_path.exists():
        issues.append(f"Modelo generador no encontrado: {generator_path}")
    
    return issues

def print_config_summary():
    """Imprime un resumen de la configuraci√≥n actual"""
    print("üîß CONFIGURACI√ìN DEL SISTEMA RAG ENRIQUECIDO")
    print("=" * 50)
    print(f"üìÅ Directorio base: {BASE_DIR.absolute()}")
    print(f"üóÉÔ∏è Base datos autoaprendizaje: {DB_AUTOAPRENDIZAJE}")
    print(f"üß† Base datos perfiles: {DB_PERFILES}")
    print(f"üìä FAISS contenido: {FAISS_A_INDEX}")
    print(f"üé≠ FAISS perfiles: {FAISS_B_INDEX}")
    print(f"ü§ñ Modelo embeddings: {EMBEDDING_MODEL}")
    print(f"‚ö° Modelo generador: {GENERATOR_MODEL_LOCAL}")
    print(f"üåê Gemini habilitado: {'‚úÖ' if USE_GEMINI else '‚ùå'}")
    print(f"üîç K b√∫squeda contenido: {DEFAULT_K_SEARCH}")
    print(f"üß≠ K b√∫squeda perfiles: {DEFAULT_K_PROFILES}")
    print(f"üåê Webapp: http://{FLASK_HOST}:{FLASK_PORT}")
    
    # Mostrar issues si existen
    issues = validate_config()
    if issues:
        print("\n‚ö†Ô∏è PROBLEMAS DETECTADOS:")
        for issue in issues:
            print(f"  ‚Ä¢ {issue}")
    else:
        print("\n‚úÖ Configuraci√≥n v√°lida")
    
    print("=" * 50)

# ==========================================================
# üîπ CONFIGURACI√ìN DE ENTORNO
# ==========================================================
def setup_environment():
    """Configura el entorno necesario para el sistema"""
    
    # Variables de entorno por defecto
    env_defaults = {
        "TOKENIZERS_PARALLELISM": "false",  # Evitar warnings de HuggingFace
        "TRANSFORMERS_CACHE": str(MODELS_DIR / "cache"),
        "HF_HOME": str(MODELS_DIR / "huggingface"),
    }
    
    for key, value in env_defaults.items():
        if key not in os.environ:
            os.environ[key] = value
    
    # Crear archivo de configuraci√≥n local si no existe
    config_file = BASE_DIR / "config.local.json"
    if not config_file.exists():
        import json
        local_config = {
            "embedding_model": EMBEDDING_MODEL,
            "generator_model": GENERATOR_MODEL_LOCAL,
            "chunk_size": DEFAULT_CHUNK_SIZE,
            "k_search": DEFAULT_K_SEARCH,
            "k_profiles": DEFAULT_K_PROFILES,
            "use_gemini": USE_GEMINI,
            "flask_port": FLASK_PORT,
            "verbose_logging": ENABLE_VERBOSE_LOGGING
        }
        
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(local_config, f, indent=2, ensure_ascii=False)
        
        print(f"üìÑ Archivo de configuraci√≥n creado: {config_file}")

# ==========================================================
# üîπ INICIALIZACI√ìN
# ==========================================================
if __name__ == "__main__":
    setup_environment()
    print_config_summary()
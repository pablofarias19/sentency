# -*- coding: utf-8 -*-
"""
===========================================================
  DETECTOR DE AUTOR Y MÃ‰TODO JURÃDICO â€“ ANALYSER MÃ‰TODO
===========================================================

Objetivos:
    1ï¸âƒ£ Detectar autor principal y distinguir autores citados.
    2ï¸âƒ£ Analizar notas al pie y bibliografÃ­a.
    3ï¸âƒ£ Clasificar tipo de razonamiento (segÃºn tÃ³picos y retÃ³rica).
    4ï¸âƒ£ Generar un perfil cognitivo extendido para integrar a FAISS / SQLite.

Dependencias:
    pip install PyMuPDF spacy regex numpy
    python -m spacy download es_core_news_md
===========================================================
"""

import re
import fitz  # PyMuPDF
import numpy as np
from pathlib import Path
from typing import Dict, List
import json

# Intentar cargar spaCy, con fallback si no estÃ¡ disponible
try:
    import spacy
    nlp = spacy.load("es_core_news_md")
    SPACY_AVAILABLE = True
except:
    print("âš ï¸ spaCy no disponible. Usando regex para NER bÃ¡sico.")
    SPACY_AVAILABLE = False

# ----------------------------------------------------------
# ðŸ”¹ 1. UTILIDADES PDF
# ----------------------------------------------------------
def extraer_texto_y_notas(ruta_pdf: str) -> Dict:
    """Extrae texto completo y notas al pie del PDF."""
    try:
        doc = fitz.open(ruta_pdf)
        texto_completo = ""
        notas_pie = []

        for page in doc:
            ph = page.rect.height
            blocks = page.get_text("blocks")
            font_sizes = [b[7] for b in blocks if len(b) >= 8]
            font_mean = np.mean(font_sizes) if font_sizes else 10

            for b in blocks:
                x0, y0, x1, y1, txt, *_ = b
                if not txt.strip():
                    continue
                # Detectar notas al pie por posiciÃ³n y tamaÃ±o de fuente
                if y0 > 0.85 * ph and (len(b) >= 8 and b[7] < font_mean - 0.5):
                    notas_pie.append(txt)
                texto_completo += txt + "\n"

        doc.close()
        return {"texto": texto_completo, "notas_pie": "\n".join(notas_pie)}
    
    except Exception as e:
        print(f"âŒ Error extrayendo PDF {ruta_pdf}: {e}")
        return {"texto": "", "notas_pie": ""}

# ----------------------------------------------------------
# ðŸ”¹ 2. DETECCIÃ“N DE AUTORES
# ----------------------------------------------------------
def detectar_autor_principal(ruta_pdf: str, texto: str) -> Dict:
    """
    Detecta el autor principal mediante portada, metadatos y NER.
    Devuelve autor y nivel de confianza.
    """
    try:
        doc = fitz.open(ruta_pdf)
        meta = doc.metadata
        primera_pagina = doc.load_page(0).get_text("text")
        doc.close()
    except:
        primera_pagina = texto[:2000]  # Fallback: primeros 2000 caracteres
        meta = {}

    candidatos = []

    # 1ï¸âƒ£ Metadatos del PDF
    if meta.get("author"):
        candidatos.append((meta["author"], 0.4))

    # 2ï¸âƒ£ Portada: "Por/Autor"
    patron_portada = r"(?i)(por|autor(?:a)?|escrito\s+por)[\s:]+([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+(?:\s+[A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+){1,3})"
    match = re.search(patron_portada, primera_pagina)
    if match:
        candidatos.append((match.group(2), 0.6))

    # 3ï¸âƒ£ Patrones de autorÃ­a en portada
    patrones_autor = [
        r"(?i)dr\.?\s+([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+(?:\s+[A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+){1,2})",
        r"(?i)prof\.?\s+([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+(?:\s+[A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+){1,2})",
        r"([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+(?:\s+[A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+){1,2})(?=\s*\n.*\d{4})"  # Nombre antes de aÃ±o
    ]
    
    for patron in patrones_autor:
        match = re.search(patron, primera_pagina)
        if match:
            candidatos.append((match.group(1), 0.7))

    # 4ï¸âƒ£ NER con spaCy si estÃ¡ disponible
    if SPACY_AVAILABLE:
        doc_nlp = nlp(primera_pagina[:1500])  # Limitar para eficiencia
        personas = [ent.text for ent in doc_nlp.ents if ent.label_ == "PER"]
        if personas:
            autor = max(personas, key=len)
            candidatos.append((autor, 0.3))

    # 5ï¸âƒ£ NER bÃ¡sico con regex si spaCy no estÃ¡ disponible
    else:
        patron_nombres = r"([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+(?:\s+[A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+){1,2})"
        nombres = re.findall(patron_nombres, primera_pagina[:1000])
        if nombres:
            # Filtrar nombres comunes/palabras no vÃ¡lidas
            nombres_filtrados = [n for n in nombres if not re.search(r"(?i)(derecho|civil|penal|cÃ³digo|ley|artÃ­culo)", n)]
            if nombres_filtrados:
                autor = max(nombres_filtrados, key=len)
                candidatos.append((autor, 0.2))

    # Seleccionar mejor candidato
    if candidatos:
        # Eliminar duplicados similares
        candidatos_unicos = []
        for autor, conf in candidatos:
            if not any(autor.lower() in c[0].lower() or c[0].lower() in autor.lower() for c in candidatos_unicos):
                candidatos_unicos.append((autor, conf))
        
        autor_final, confianza = max(candidatos_unicos, key=lambda x: x[1])
        return {"autor_principal": autor_final.strip(), "confianza": round(confianza, 2)}
    
    return {"autor_principal": "Autor no identificado", "confianza": 0.0}

def extraer_autores_citados(texto: str, notas: str) -> List[Dict]:
    """
    Extrae autores doctrinarios o citados desde texto y notas al pie.
    """
    patrones = [
        r"([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+),\s*([A-ZÃÃ‰ÃÃ“ÃšÃ‘]\.(?:\s*[A-ZÃÃ‰ÃÃ“ÃšÃ‘]\.)*)",  # Apellido, N. N.
        r"(?i)(?:segÃºn|conforme|cfr\.?|ver|vid\.?)\s+([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+(?:\s+[A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+)?)",
        r"([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+(?:\s+[A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+)?)\s*\(\s*\d{4}\s*\)",  # Autor (aÃ±o)
        r"(?i)doctrina(?:riamente)?\s+([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+)",
        r"(?i)enseÃ±a\s+([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+)",
        r"(?i)sostiene\s+([A-ZÃÃ‰ÃÃ“ÃšÃ‘][a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+)"
    ]

    texto_combinado = texto + "\n" + notas
    coincidencias = []
    
    for patron in patrones:
        matches = re.findall(patron, texto_combinado)
        for match in matches:
            if isinstance(match, tuple):
                # Para patrones con grupos mÃºltiples, tomar el primer grupo vÃ¡lido
                autor = next((m for m in match if m and len(m) > 2), None)
            else:
                autor = match
            
            if autor:
                coincidencias.append(autor)

    # Filtrar y limpiar autores
    autores_filtrados = []
    for autor in set(coincidencias):
        autor = autor.strip()
        # Filtrar palabras que no son nombres de autores
        if (len(autor) > 3 and 
            not re.search(r"(?i)(artÃ­culo|cÃ³digo|ley|derecho|civil|penal|segÃºn|conforme)", autor) and
            re.search(r"[A-ZÃÃ‰ÃÃ“ÃšÃ‘]", autor)):
            
            ubicacion = "nota_pie" if autor in notas else "texto"
            autores_filtrados.append({"autor_citado": autor, "ubicacion": ubicacion})

    return autores_filtrados[:20]  # Limitar a 20 para evitar ruido

# ----------------------------------------------------------
# ðŸ”¹ 3. CLASIFICACIÃ“N DE RAZONAMIENTO
# ----------------------------------------------------------
def clasificar_razonamiento(texto: str) -> List[Dict]:
    """
    Clasifica el tipo de razonamiento jurÃ­dico segÃºn tÃ³picos aristotÃ©licos.
    Devuelve top 3 con scores.
    """
    texto_lower = texto.lower()

    patrones = {
        "Deductivo": [
            r"en consecuencia", r"por tanto", r"se concluye que", r"por ende",
            r"de ello se desprende", r"resulta que", r"se sigue que"
        ],
        "Inductivo": [
            r"por ejemplo", r"v\.gr\.", r"casos?", r"se desprende que",
            r"en base a", r"a partir de", r"considerando que"
        ],
        "AnalÃ³gico": [
            r"por analog(Ã­|i)a", r"semejanza", r"comparable", r"similar",
            r"del mismo modo", r"paralelamente", r"mutatis mutandis"
        ],
        "TeleolÃ³gico": [
            r"finalidad", r"funciÃ³n", r"propÃ³sito", r"utilidad social",
            r"ratio legis", r"espÃ­ritu de la ley", r"bien jurÃ­dico"
        ],
        "SistÃ©mico": [
            r"sistema", r"estructura", r"coherencia", r"subsistema",
            r"ordenamiento", r"conjunto normativo", r"unidad del derecho"
        ],
        "Autoritativo": [
            r"segÃºn", r"conforme", r"doctrina", r"jurisprudencia", r"fallos",
            r"art\.", r"artÃ­culo", r"tribunal", r"corte", r"csjn"
        ],
        "A contrario": [
            r"a contrario", r"a sensu contrario", r"salvo", r"excepto",
            r"por el contrario", r"inversamente", r"en sentido opuesto"
        ],
        "Consecuencialista": [
            r"consecuencia", r"efecto", r"impacto", r"resultado", r"beneficio",
            r"perjuicio", r"ventaja", r"inconveniente"
        ]
    }

    pesos = {
        "Deductivo": 1.0, "Autoritativo": 0.9, "TeleolÃ³gico": 0.8, "SistÃ©mico": 0.8,
        "Inductivo": 0.7, "AnalÃ³gico": 0.6, "A contrario": 0.6, "Consecuencialista": 0.5
    }

    scores = {}
    for tipo, expresiones in patrones.items():
        count = sum(len(re.findall(expr, texto_lower)) for expr in expresiones)
        # Normalizar por longitud del texto
        score = (count / len(texto.split()) * 1000) * pesos[tipo] if texto.split() else 0
        scores[tipo] = min(score, 1.0)  # Limitar a 1.0

    top3 = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:3]
    return [{"clase": c, "score": round(s, 3)} for c, s in top3 if s > 0]

def detectar_componentes_retÃ³ricos(texto: str) -> Dict:
    """
    EvalÃºa la presencia de Ethos, Pathos y Logos (retÃ³rica aristotÃ©lica).
    """
    t = texto.lower()
    
    # Ethos: Autoridad, credibilidad
    ethos_patterns = [
        r"doctrina", r"autoridad", r"tribunal", r"jurisprudencia", r"csjn", r"scba",
        r"profesor", r"doctor", r"especialista", r"experto", r"catedrÃ¡tico"
    ]
    ethos = sum(len(re.findall(p, t)) for p in ethos_patterns)
    
    # Pathos: EmociÃ³n, valores
    pathos_patterns = [
        r"injusticia", r"grave", r"alarmante", r"daÃ±o", r"moral", r"Ã©tico",
        r"inadmisible", r"repudiable", r"lesivo", r"perjudicial", r"dramÃ¡tico"
    ]
    pathos = sum(len(re.findall(p, t)) for p in pathos_patterns)
    
    # Logos: LÃ³gica, razÃ³n
    logos_patterns = [
        r"porque", r"por tanto", r"si", r"entonces", r"en consecuencia", r"segÃºn",
        r"debido a", r"en virtud de", r"fundamentalmente", r"racionalmente"
    ]
    logos = sum(len(re.findall(p, t)) for p in logos_patterns)
    
    total = ethos + pathos + logos or 1
    return {
        "ethos": round(ethos / total, 3),
        "pathos": round(pathos / total, 3),
        "logos": round(logos / total, 3)
    }

# ----------------------------------------------------------
# ðŸ”¹ 4. ANÃLISIS ADICIONAL
# ----------------------------------------------------------
def analizar_complejidad_sintactica(texto: str) -> float:
    """Analiza la complejidad sintÃ¡ctica del texto."""
    oraciones = re.split(r'[.!?]+', texto)
    if not oraciones:
        return 0.0
    
    palabras_por_oracion = [len(oracion.split()) for oracion in oraciones if oracion.strip()]
    if not palabras_por_oracion:
        return 0.0
    
    complejidad = np.mean(palabras_por_oracion) / 20.0  # Normalizar
    return min(round(complejidad, 3), 1.0)

def detectar_nivel_tecnico(texto: str) -> Dict:
    """Detecta el nivel tÃ©cnico del documento."""
    t = texto.lower()
    
    # TÃ©rminos tÃ©cnicos jurÃ­dicos
    terminos_tecnicos = [
        r"ratio decidendi", r"obiter dicta", r"res iudicata", r"ultra petita",
        r"iura novit curia", r"ne bis in idem", r"habeas corpus", r"mandamus",
        r"certiorari", r"amicus curiae", r"stare decisis", r"per se"
    ]
    
    # Latinismos
    latinismos = sum(len(re.findall(p, t)) for p in terminos_tecnicos)
    
    # Citas de artÃ­culos y leyes
    citas_legales = len(re.findall(r"art(?:Ã­culo)?\.?\s*\d+", t))
    
    # Referencias doctrinarias
    referencias = len(re.findall(r"(?:cfr\.|ver|vid\.|segÃºn)", t))
    
    total_palabras = len(texto.split())
    if total_palabras == 0:
        return {"nivel_tecnico": 0.0, "latinismos": 0, "citas_legales": 0, "referencias": 0}
    
    nivel = min((latinismos + citas_legales + referencias) / total_palabras * 100, 1.0)
    
    return {
        "nivel_tecnico": round(nivel, 3),
        "latinismos": latinismos,
        "citas_legales": citas_legales,
        "referencias": referencias
    }

# ----------------------------------------------------------
# ðŸ”¹ 5. PERFIL COGNITIVO EXTENDIDO
# ----------------------------------------------------------
def generar_perfil_cognitivo_extendido(ruta_pdf: str) -> Dict:
    """
    Extrae texto, notas, autores, razonamiento y componentes retÃ³ricos.
    Genera un perfil cognitivo completo e integrable.
    """
    print(f"ðŸ” Analizando: {Path(ruta_pdf).name}")
    
    # Extraer contenido
    data = extraer_texto_y_notas(ruta_pdf)
    texto = data["texto"]
    notas = data["notas_pie"]
    
    if not texto.strip():
        print("âš ï¸ No se pudo extraer texto del PDF")
        return {"error": "No se pudo procesar el PDF"}
    
    print(f"ðŸ“„ Texto extraÃ­do: {len(texto)} caracteres")
    print(f"ðŸ“ Notas al pie: {len(notas)} caracteres")
    
    # AnÃ¡lisis principal
    autor_ppal = detectar_autor_principal(ruta_pdf, texto)
    autores_citados = extraer_autores_citados(texto, notas)
    razonamiento = clasificar_razonamiento(texto)
    retorica = detectar_componentes_retÃ³ricos(texto)
    complejidad = analizar_complejidad_sintactica(texto)
    nivel_tecnico = detectar_nivel_tecnico(texto)
    
    # Construir perfil integrado
    perfil = {
        "metadata": {
            "archivo": Path(ruta_pdf).name,
            "total_palabras": len(texto.split()),
            "total_caracteres": len(texto),
            "notas_pie_detectadas": len(notas.split("\n")) if notas else 0
        },
        "autor_principal": autor_ppal,
        "autores_citados": autores_citados,
        "razonamiento_top3": razonamiento,
        "retorica": retorica,
        "complejidad_sintactica": complejidad,
        "nivel_tecnico": nivel_tecnico,
        "timestamp": str(np.datetime64('now'))
    }
    
    print(f"âœ… AnÃ¡lisis completado:")
    print(f"   ðŸ‘¤ Autor: {autor_ppal['autor_principal']} (confianza: {autor_ppal['confianza']})")
    print(f"   ðŸ“š Autores citados: {len(autores_citados)}")
    print(f"   ðŸ§­ Razonamiento principal: {razonamiento[0]['clase'] if razonamiento else 'No detectado'}")
    print(f"   ðŸŽ­ RetÃ³rica dominante: {max(retorica, key=retorica.get) if retorica else 'No detectada'}")
    
    return perfil

# ----------------------------------------------------------
# ðŸ”¹ 6. FUNCIONES DE INTEGRACIÃ“N
# ----------------------------------------------------------
def integrar_con_perfil_cognitivo_existente(perfil_extendido: Dict, perfil_base: Dict) -> Dict:
    """
    Integra el perfil extendido con un perfil cognitivo base existente.
    """
    perfil_integrado = perfil_base.copy()
    
    # Agregar campos del perfil extendido
    perfil_integrado.update({
        "autor_principal": perfil_extendido.get("autor_principal", {}),
        "autores_citados": perfil_extendido.get("autores_citados", []),
        "razonamiento_dominante": perfil_extendido.get("razonamiento_top3", [{}])[0].get("clase", "No detectado"),
        "ethos": perfil_extendido.get("retorica", {}).get("ethos", 0),
        "pathos": perfil_extendido.get("retorica", {}).get("pathos", 0),
        "logos": perfil_extendido.get("retorica", {}).get("logos", 0),
        "complejidad_sintactica": perfil_extendido.get("complejidad_sintactica", 0),
        "nivel_tecnico": perfil_extendido.get("nivel_tecnico", {}).get("nivel_tecnico", 0)
    })
    
    return perfil_integrado

def exportar_para_sqlite(perfil_extendido: Dict) -> Dict:
    """
    Prepara el perfil para inserciÃ³n en SQLite.
    """
    return {
        "autor_principal": perfil_extendido.get("autor_principal", {}).get("autor_principal", ""),
        "autor_confianza": perfil_extendido.get("autor_principal", {}).get("confianza", 0.0),
        "autores_citados": json.dumps(perfil_extendido.get("autores_citados", []), ensure_ascii=False),
        "razonamiento_top3": json.dumps(perfil_extendido.get("razonamiento_top3", []), ensure_ascii=False),
        "ethos": perfil_extendido.get("retorica", {}).get("ethos", 0.0),
        "pathos": perfil_extendido.get("retorica", {}).get("pathos", 0.0),
        "logos": perfil_extendido.get("retorica", {}).get("logos", 0.0),
        "complejidad_sintactica": perfil_extendido.get("complejidad_sintactica", 0.0),
        "nivel_tecnico": perfil_extendido.get("nivel_tecnico", {}).get("nivel_tecnico", 0.0)
    }

# ----------------------------------------------------------
# ðŸ”¹ 7. USO DIRECTO
# ----------------------------------------------------------
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("\n" + "="*60)
        print("ðŸ§  DETECTOR DE AUTOR Y MÃ‰TODO JURÃDICO â€“ ANALYSER MÃ‰TODO")
        print("="*60)
        print("\nUso: python detector_autor_y_metodo.py archivo.pdf")
        print("\nEjemplo:")
        print("  python detector_autor_y_metodo.py documento_juridico.pdf")
        print("\nEste mÃ³dulo integra con tu sistema ANALYSER para:")
        print("  â€¢ Detectar autores principales y citados")
        print("  â€¢ Clasificar tipos de razonamiento jurÃ­dico")
        print("  â€¢ Analizar componentes retÃ³ricos (Ethos/Pathos/Logos)")
        print("  â€¢ Generar perfiles cognitivos extendidos")
        sys.exit(0)

    ruta = sys.argv[1]
    if not Path(ruta).exists():
        print(f"âŒ Archivo no encontrado: {ruta}")
        sys.exit(1)

    print(f"\nðŸ“˜ ANÃLISIS MÃSTER - ANALYSER MÃ‰TODO")
    print("="*60)
    
    try:
        perfil = generar_perfil_cognitivo_extendido(ruta)
        
        if "error" in perfil:
            print(f"âŒ {perfil['error']}")
            sys.exit(1)
        
        print(f"\nðŸ§  PERFIL COGNITIVO EXTENDIDO")
        print("-"*40)
        
        # Mostrar resultados organizados
        print(f"\nðŸ‘¤ AUTORÃA:")
        autor_info = perfil['autor_principal']
        print(f"   Autor principal: {autor_info['autor_principal']}")
        print(f"   Confianza: {autor_info['confianza']}")
        
        if perfil['autores_citados']:
            print(f"\nðŸ“š AUTORES CITADOS ({len(perfil['autores_citados'])}):")
            for autor in perfil['autores_citados'][:5]:  # Mostrar primeros 5
                print(f"   â€¢ {autor['autor_citado']} ({autor['ubicacion']})")
        
        if perfil['razonamiento_top3']:
            print(f"\nðŸ§­ RAZONAMIENTO JURÃDICO:")
            for i, r in enumerate(perfil['razonamiento_top3'], 1):
                print(f"   {i}. {r['clase']}: {r['score']}")
        
        print(f"\nðŸŽ­ RETÃ“RICA ARISTOTÃ‰LICA:")
        ret = perfil['retorica']
        print(f"   Ethos (autoridad): {ret['ethos']}")
        print(f"   Pathos (emociÃ³n): {ret['pathos']}")
        print(f"   Logos (lÃ³gica): {ret['logos']}")
        
        print(f"\nðŸ“Š MÃ‰TRICAS ADICIONALES:")
        print(f"   Complejidad sintÃ¡ctica: {perfil['complejidad_sintactica']}")
        print(f"   Nivel tÃ©cnico: {perfil['nivel_tecnico']['nivel_tecnico']}")
        print(f"   Latinismos: {perfil['nivel_tecnico']['latinismos']}")
        print(f"   Citas legales: {perfil['nivel_tecnico']['citas_legales']}")
        
        # Exportar resultado completo
        output_file = Path(ruta).stem + "_perfil_extendido.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(perfil, f, indent=2, ensure_ascii=False)
        
        print(f"\nðŸ’¾ Perfil completo guardado en: {output_file}")
        print("\nâœ… AnÃ¡lisis completado exitosamente!")
        
    except Exception as e:
        print(f"\nâŒ Error durante el anÃ¡lisis: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)